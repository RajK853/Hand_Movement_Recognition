{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test model to classify hand movements\n",
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the standard python modules used here in this program.\n",
    "* __os:__ To find certain directories and to generate file/directory full address\n",
    "* __numpy:__ To handle numpy arrays\n",
    "* __collections:__ To use the Counter class to show the number of each gestures used for training and testing the model\n",
    "* __matplotlib:__ To plot the graph of the acceleration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import keras module and other required classes from it that are required to build our Artificial Neural Network and to plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"src\")\n",
    "from Utils import load_processed_data, execute_layers, LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Utils__ module contains some custom functions and is located in the __src__ directory of our project. By adding __src__ in the system path, we can import the modules from it directly.\n",
    "Description of the functions loaded from __Utils__ modules are as follows:\n",
    "* __load_processed_data:__ Loads a processed data from a given csv file where each row contains the x-, y-, and z-acceleration data stacked one after another and followed by its label. The values are normalized in the range 0-1 inclusively and smoothed using a moving average window.\n",
    "*  __execute_layers:__ This function takes a list of inputs and layers as arguments and passes each input through all the layers and returns a list of outputs from each input.\n",
    "\n",
    "__LABELS__ is a tuple with labels of the gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train and test data\n",
    "Number of sample points of the gestures are not same. Therefore, we need to pad them to achieve a given constant length for all gestures. __max_review_length__ denotes the maximum length of the sample points after padding them either at __\"start\"__ with initial value or at __\"end\"__ with final value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 150\n",
    "data_train, labels_train, data_test, labels_test = load_processed_data(\"Processed Data 12.06.2019 00.05.csv\", \n",
    "                                                                       review_length=max_review_length, \n",
    "                                                                       pad_pos=\"start\",\n",
    "                                                                       train_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting accelerations\n",
    "Here we plot one graph for each gesture. The graph shows the x-, y- and z-acceleration readings of BBC Microbit against time.\n",
    "\n",
    "__Acceleration values are normalized within the range 0-1 inclusively.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PERIOD = 0.01\n",
    "NUM_LABELS = len(LABELS)\n",
    "start_index = 0                                           # Initial index\n",
    "last_index  = len(data_train[0])-1                        # Final index = Length of data-1\n",
    "\n",
    "# Matplot plot formatting\n",
    "plt.figure(figsize=(18.0, 10.0))                          # Setting plot figure size\n",
    "plt.subplots_adjust(hspace=0.5)                           # Adjusting hrizontal spacing between subplots\n",
    "t = np.arange(max_review_length)*SAMPLE_PERIOD            # Time range for each plot\n",
    "figures = []\n",
    "for i in range(NUM_LABELS):\n",
    "    # Select offset of unique data i.e data that is not plotted yet\n",
    "    for index in range(start_index, last_index):          # Offset = (start_index, last_index)\n",
    "        label_index = labels_train[index+i].argmax()\n",
    "        if label_index not in figures:                    # Given gesture type not plotted?\n",
    "            figures.append(label_index)\n",
    "            start_index = index+1                         # Start loop next time from next index \n",
    "            break\n",
    "    else:\n",
    "        print(\"✘ Not enough data: Only {} different plots drawn.\".format(i))\n",
    "        break\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.title(LABELS[label_index].capitalize())\n",
    "    plt.xlabel(\"time (s)\")\n",
    "    plt.ylabel(\"Normalized acceleration\")\n",
    "    x, y, z = data_train[:, index+i, 0]\n",
    "    plt.plot(t, x, \"r\", label=\"x acceleration\")\n",
    "    plt.plot(t, y, \"g\", label=\"y acceleration\")\n",
    "    plt.plot(t, z, \"b\", label=\"z acceleration\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers and Model initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialise our inputs i.e acceleration readings in x, y and z planes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = Input(shape=(1, max_review_length), name=\"Acceleration_x\")\n",
    "input_y = Input(shape=(1, max_review_length), name=\"Acceleration_y\")\n",
    "input_z = Input(shape=(1, max_review_length), name=\"Acceleration_z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our neural network, we have intialized LSTM with dropout and Dense layers. **tanh** activation function in LSTM layer and **ReLu** activation functions in Dense layers seems to produce better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_layers = (LSTM(max_review_length, activation=\"tanh\", name=\"Shared_LSTM\", dropout=0.75),\n",
    "                 Dense(NUM_LABELS*3*64,  activation=\"relu\", name=\"Shared_Dense_1\"),\n",
    "                 Dense(NUM_LABELS*3*64,  activation=\"relu\", name=\"Shared_Dense_2\"),\n",
    "                 Dense(NUM_LABELS*1*64,  activation=\"relu\", name=\"Shared_Dense_3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM and dense layers are shared by each input; for a single gesture, we train the each layer with the x, y and z input acceleration inputs.\n",
    "The **execute_layers()** function passes each input through all these layers and returns an array with output from each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_output = execute_layers(inputs=(input_x, input_y, input_z), layers=shared_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the inputs (x, y and z) are concatenated and passed further through Dense layers where the final Dense layer is our softmax classification layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat      = keras.layers.concatenate(shared_output,name=\"Concatenate\")\n",
    "dense_1     = Dense(39, activation=\"relu\",    name=\"Dense_1\")(concat)\n",
    "main_output = Dense(NUM_LABELS,   activation=\"softmax\", name=\"Classification_Layer\")(dense_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating, training and testing model\n",
    "The model with above inputs and outputs is created. It is then trained using the processed training data and tested with the processed testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=(input_x, input_y, input_z), outputs=main_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "print(\"Model Summary\", model.summary(), sep=\"\\n\")\n",
    "history = model.fit(x=[*data_train], y=labels_train, epochs=10, batch_size=10)\n",
    "\n",
    "print(\"\\nTesting\")\n",
    "scores = model.evaluate(x=[*data_test], y=labels_test, batch_size=10, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and loss values graph\n",
    "Below we can see the graph of **accuracy** and **loss** values of the model against **epoch** while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Accuracy', 'Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "Using confusion matrix, we can observe the performance of our model while testing it with our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"ideal\", \"up\", \"down\", \"left\", \"right\"]\n",
    "y_pred = [labels[i] for i in np.argmax(model.predict([*data_test]), axis=1)]\n",
    "y_test = y_true = [labels[i] for i in np.argmax(labels_test, axis=1)]\n",
    "y_train = [labels[i] for i in np.argmax(labels_train, axis=1)]\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.terrain_r)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.xticks(np.arange(len(labels)), labels)\n",
    "plt.yticks(np.arange(len(labels)), labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the confusion martix are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\", \"\\n\".join(map(str, cm)), sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for **left** and **right** movements usually have more false predictions.\n",
    "Our hypothesis for this is that vertical movements are easier compared to horizontal movements.\n",
    "Thus the data collected for left and right movements are not so consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the number of data we used from each gesture to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data:\", Counter(y_train))\n",
    "print(\"Test  data:\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving trained model\n",
    "Once our model is trained and tested, we need to save the trained model. The model is saved if the accuracy of the model is atleast the accuracy threshold value. \n",
    "\n",
    "The model name indicates the date and time when the model was saved and the model can be found in the **Models** directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy_threshold = 0.96\n",
    "if scores[1] >= accuracy_threshold:\n",
    "    model_file = \"Model {accuracy:.2f} {date}.HDF5\".format(\n",
    "        date=datetime.now().strftime(\"%d.%m.%Y %H.%M\"), accuracy=(scores[1]*100))\n",
    "    if not os.path.exists(\"Models\"):\n",
    "        os.makedirs(\"Models\")\n",
    "    file_path = os.path.join(\"Models\", model_file)\n",
    "    model.save(file_path)\n",
    "    print(\"✓ Model saved: {}\".format(file_path))\n",
    "else:\n",
    "    print(\"✘ Model not saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
